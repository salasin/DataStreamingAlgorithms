{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Streaming Algorithms\n",
    "\n",
    "## HW2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See preprocessed notebook at https://github.com/salasin/DataStreamingAlgorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dry Part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$M$ is a data stream and $|M| = n$.\n",
    "\n",
    "$A$ is $M$'s heavy hitters sketch and $|A| = k$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) For every $j ∈ A, \\tilde{f_j} \\ge f_j$\n",
    "\n",
    "The proof is by induction on the number of appearances of an element in the stream.\n",
    "\n",
    "**Base case:**\n",
    "\n",
    "After the 1st appearance of $j$ we have $f_j = 1$.\n",
    "\n",
    "If $|A| < k$ before $j$'s appearance then it's added to the sketch and we get $\\tilde{f_j} = 1 \\ge f_j$.\n",
    "\n",
    "Otherwise, if $|A| = k$ then we get $\\tilde{f_j} = 1 + \\tilde{f_{}}_{min} \\ge 1 = f_j$ because $\\tilde{f_{}}_{min} \\ge 0$ because the counters are non-negative.\n",
    "\n",
    "**Induction step:**\n",
    "\n",
    "We denote by $\\tilde{f_j}^{(i)}$ the value of $j$'s counter after $i$ appearances of $j$ and by $f_j^{(i)}$ the true count of $j$'s appearances (which is $i$).\n",
    "\n",
    "We assume that $\\tilde{f_j}^{(i)} \\ge f_j^{(i)}$ holds for a given $i \\ge 1$.\n",
    "\n",
    "Then for $i + 1$ we get:\n",
    "\n",
    "If $j \\in A$ then we incremenet $\\tilde{f_j}^{(i)}$ and we get $\\tilde{f_j}^{(i + 1)} = \\tilde{f_j}^{(i)} + 1 \\ge f_j^{(i)} + 1 = f_j^{(i + 1)}$.\n",
    "\n",
    "If $j \\notin A$ and since $i \\ge 1$ (not $j$'s first appearance) we can infer that $j$ was in $A$ and was removed. Therefore it must hold that $|A| = k$. At the time of $j$'s removal from $A$ we had $\\tilde{f_j}^{(i)} = \\tilde{f_{}}_{min}$. Now we want to re-insert $j$ to $A$ and we have a new $\\tilde{f_{}}_{min}^{(new)}$ that is bigger than $\\tilde{f_{}}_{min}$ because all counters are monotonically increasing. Then we get $\\tilde{f_j}^{(i + 1)} = \\tilde{f_{}}_{min}^{(new)} + 1 > \\tilde{f_{}}_{min} + 1 = \\tilde{f_j}^{(i)} + 1 \\ge f_j^{(i)} + 1 = f_j^{(i + 1)}$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) For every $j \\in [M], \\tilde{f_j} ≤ f_j + \\tilde{f_{}}_{min}$\n",
    "\n",
    "If $j \\notin A$ we have $\\tilde{f_j} = 0$ and the inequality holds since the right side is non-negative.\n",
    "\n",
    "If $j \\in A$, then $j$ was inserted to $A$ at some point and additional instances of $j$ may have been observed since. We denote with $\\tilde{f_j}^{(curr)}$ the current value of the counter corresponding to $j$. We denote with $\\tilde{f_{}}_{min}^{(curr)}$ the value of the counter that currently has the minimal value and with $\\tilde{f_{}}_{min}^{(old)}$ the value of the counter that had the minimal value at the time $j$ was inserted. We denote with $f_{j}^{(before)}$ the count of $j$ instances before $j$'s insertion and with $f_{j}^{(after)}$ the similar count after the insertion.\n",
    "\n",
    "We observe that $f_j = f_{j}^{(before)} + f_{j}^{(after)}$ and therefore $f_j \\ge f_{j}^{(after)}$. We also observe that $\\tilde{f_{}}_{min}^{(old)} \\le \\tilde{f_{}}_{min}^{(curr)}$ because all counters are monotonically increasing.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\tilde{f_j} = \\tilde{f_j}^{(curr)} = f_{j}^{(after)} + \\tilde{f_{}}_{min}^{(old)} \\le f_j + \\tilde{f_{}}_{min}^{(curr)} = f_j + \\tilde{f_{}}_{min}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) $ֿ\\sum_{j \\in A} \\tilde{f_j} = n$\n",
    "\n",
    "Proof by induction on the size of the stream.\n",
    "\n",
    "**Base case:**\n",
    "\n",
    "When n = 1, i.e. when we process the first element $i$ in the stream, we add a new counter $\\tilde{f_i} = 1$ to the sketch. Therefore:\n",
    "\n",
    "$ֿ\\sum_{j \\in A} \\tilde{f_j} = \\tilde{f_i} = 1 = n$\n",
    "\n",
    "**Induction step:**\n",
    "\n",
    "We denote by $\\sum_{j \\in A}^{(n)} \\tilde{f_j}$ the sum of all counters and by $\\tilde{f_{j}}^{(n)}$ the value of $j$'s counter after $n$ elements from the stream were processed.\n",
    "\n",
    "We assume that $ֿ\\sum_{j \\in A}^{(n)} \\tilde{f_j} = n$ holds for a given $n$.\n",
    "\n",
    "Then for $n + 1$ we get:\n",
    "\n",
    "We start processing the $n + 1$ element $i$ so we incremenet $\\tilde{f_i}$ (Note that it doesn't matter in this context whether $i$ was in $A$. If it wasn't we simply assume that before the incrementation $\\tilde{f_i} = \\tilde{f}_{min}$) and we get:\n",
    "\n",
    " $\\sum_{j \\in A}^{(n + 1)} \\tilde{f_j} = ֿ\\sum_{j \\in A, j \\neq i}^{(n + 1)} \\tilde{f_j} + \\tilde{f_i}^{(n + 1)} = \\sum_{j \\in A, j \\neq i}^{(n + 1)} \\tilde{f_j} + \\tilde{f_i}^{(n)} + 1 = \\sum_{j \\in A}^{(n)} \\tilde{f_j} + 1 = n + 1$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(d) $\\tilde{f}_{min} \\le \\lfloor n/k \\rfloor$ and hence $f_j \\le \\tilde{f_j} \\le f_j + \\lfloor n/k \\rfloor$ for every $j \\in A$.\n",
    "\n",
    "From (c) we know that $ֿ\\sum_{j \\in A} \\tilde{f_j} = n$. Also, the number of counters is $k$. Therefore, the average value of a counter is $\\lfloor n / k \\rfloor$. Since $\\tilde{f}_{min}$ is the smallest of all counters then it must be below the average and therefore $\\tilde{f}_{min} \\le \\lfloor n/k \\rfloor$.\n",
    "\n",
    "Combining this conclusion with the conclusions from (a) and (b) we can deduce that $f_j \\le \\tilde{f_j} \\le f_j + \\lfloor n/k \\rfloor$ for every $j \\in A$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wet Part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/3t/8h_yqh_915d0rs204p_w7lw400hp1f/T/ipykernel_35919/1009664826.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'2017-07-03.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0msource_ips\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ipv4Src'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0munique_source_ips\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msource_ips\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munique\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'numer of unique source ips: {unique_source_ips}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    486\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    487\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mparser\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 488\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    489\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    490\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1045\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1046\u001B[0m         \u001B[0mnrows\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidate_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"nrows\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1047\u001B[0;31m         \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1048\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1049\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    221\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    222\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlow_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 223\u001B[0;31m                 \u001B[0mchunks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_low_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    224\u001B[0m                 \u001B[0;31m# destructive to chunks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_concatenate_chunks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001B[0m in \u001B[0;36mis_extension_array_dtype\u001B[0;34m(arr_or_dtype)\u001B[0m\n\u001B[1;32m   1418\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1419\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1420\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mis_extension_array_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marr_or_dtype\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1421\u001B[0m     \"\"\"\n\u001B[1;32m   1422\u001B[0m     \u001B[0mCheck\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0man\u001B[0m \u001B[0mobject\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0mextension\u001B[0m \u001B[0marray\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('2017-07-03.csv')\n",
    "source_ips = data.loc[:, 'ipv4Src']\n",
    "unique_source_ips = len(source_ips.unique())\n",
    "print(f'numer of unique source ips: {unique_source_ips}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_frequencies = {}\n",
    "for source_ip in source_ips:\n",
    "    true_frequencies[source_ip] = true_frequencies.get(source_ip, 0) + 1\n",
    "print(f'true frequency of 35.160.100.86 is: {true_frequencies[\"35.160.100.86\"]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_elephant_flows = sorted(true_frequencies, key=true_frequencies.get, reverse=True)[:20]\n",
    "top_rare_flows = sorted(true_frequencies, key=true_frequencies.get, reverse=False)[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_hash_functions(stream, d, w):\n",
    "    # Generates perfect hash functions from source ips to values between 0 to w - 1.\n",
    "    hash_functions = []\n",
    "    for _ in range(d):\n",
    "        hash_functions.append(dict(map(lambda key, value: (key, value), stream.unique(), np.random.randint(0, w, len(stream.unique())))))\n",
    "    return hash_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_morris_counters(morris_sigma, morris_delta):\n",
    "    return int(1 / (morris_delta * (morris_sigma ** 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_avg_bias(items, sketch, true_frequencies):\n",
    "    unique_items = items.unique()\n",
    "    bias_per_item = []\n",
    "    for item in unique_items:\n",
    "        true_frequency = true_frequencies[item]\n",
    "        estimated_frequency = sketch.query(item)\n",
    "        bias = float(estimated_frequency / true_frequency)\n",
    "        normalized_bias = 1 - bias if bias <= 1 else bias - 1\n",
    "        bias_per_item.append(normalized_bias)\n",
    "    return round(sum(bias_per_item) / len(bias_per_item), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count Min Sketch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CountMinSketch:\n",
    "    def __init__(self, stream, d, w, morris_sigma=0.5, morris_delta=0.9, use_morris_counters=True):\n",
    "        self.d = d\n",
    "        self.w = w\n",
    "        self.morris_estimators_num = count_morris_counters(morris_sigma, morris_delta)\n",
    "        self.use_morris_counters = use_morris_counters\n",
    "        self.hash_functions = init_hash_functions(stream, d, w)\n",
    "        self.sketch = self.create_sketch(stream)\n",
    "\n",
    "    def create_sketch(self, stream):\n",
    "        sketch = {}\n",
    "        for i, item in enumerate(stream):\n",
    "            for j, hash_function in enumerate(self.hash_functions):\n",
    "                if self.use_morris_counters:\n",
    "                    morris_estimators = sketch.get((j, hash_function[item]), np.zeros(self.morris_estimators_num))\n",
    "                    p = 1 / (2 ** morris_estimators)\n",
    "                    update_mask = np.random.uniform(0, 1, size=morris_estimators.shape) <= p\n",
    "                    morris_estimators[update_mask] += 1\n",
    "                    sketch[(j, hash_function[item])] = morris_estimators\n",
    "                else:\n",
    "                    sketch[(j, hash_function[item])] = sketch.get((j, hash_function[item]), 0) + 1\n",
    "            sys.stdout.write(f'\\rprocessing element {i} of {len(stream)}: {round(float(i / len(stream)) * 100, 2)}% completed.')\n",
    "        print()\n",
    "        return sketch\n",
    "\n",
    "    def query(self, item):\n",
    "        if self.use_morris_counters:\n",
    "            hash_values = []\n",
    "            for i, hash_function in enumerate(self.hash_functions):\n",
    "                morris_estimators = self.sketch[(i, hash_function[item])]\n",
    "                processed_morris_estimators = (2 ** morris_estimators) - 1\n",
    "                hash_values.append(np.mean(processed_morris_estimators))\n",
    "            return min(hash_values)\n",
    "        else:\n",
    "            hash_values = [self.sketch[(i, hash_function[item])] for i, hash_function in enumerate(self.hash_functions)]\n",
    "            return min(hash_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function calculates whether the theoretical guarantee for count min sketch holds by counting the number of elements that are in the range and comparing to the total number of elements:\n",
    "\n",
    "If $w = \\dfrac{2}{\\epsilon}$ and $d = log\\dfrac{1}{\\delta}$ then $Pr[f_x \\le \\tilde{f}_x \\le f_x + \\epsilon F_1] \\ge 1 - \\delta$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_count_min_sketch_theoretical_guarantees_accuracy(items, sketch, true_frequencies):\n",
    "    unique_items = items.unique()\n",
    "    items_that_hold_range_guarantee = 0\n",
    "    epsilon = float(2 / sketch.w)\n",
    "    delta = float(1 / (2 ** sketch.d))\n",
    "    for item in unique_items:\n",
    "        true_frequency = true_frequencies[item]\n",
    "        estimated_frequency = sketch.query(item)\n",
    "        if true_frequency <= estimated_frequency <= true_frequency + epsilon * len(items):\n",
    "            items_that_hold_range_guarantee += 1\n",
    "    return round(float(items_that_hold_range_guarantee / len(unique_items)), 2), 1 - delta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to focus the problem we have, we assume that we're given a business requirement to reduce the memory consumption to 10% of the solution that uses true counters.\n",
    "\n",
    "Since we have 9640 unique elements in the stream and assuming a single counter size is 8 bytes, we need 77,120 bytes to store all the counters. We will try to reduce it to 8KB = 8192 bytes which approximately meets the business requirements.\n",
    "\n",
    "We will try to optimize the sketch accuracy and overall runtime using only 8 KB."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using regular counters, we can use up to $2^{10}$ counters:\n",
    "1. d = 1, w = 1024\n",
    "2. d = 2, w = 512\n",
    "3. d = 4, w = 256\n",
    "4. d = 8, w = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile('count_min_regular_counters_results.pkl'):\n",
    "    with open('count_min_regular_counters_results.pkl', 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "else:\n",
    "    results_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not results_dict:\n",
    "    d_w_pairs = [(1, 1024), (2, 512), (4, 256), (8, 128)]\n",
    "    elapsed_time = []\n",
    "    total_avg_bias = []\n",
    "    elephant_flows_avg_bias = []\n",
    "    rare_flows_avg_bias = []\n",
    "    theoretical_guarantee_empiric_accuracies = []\n",
    "    theoretical_guarantee_calculated_accuracies = []\n",
    "    for d, w in d_w_pairs:\n",
    "        print(f'calculating count min sketch with regular counters with d={d} and w={w}')\n",
    "        start = time.time()\n",
    "        count_min_sketch = CountMinSketch(source_ips, d, w, use_morris_counters=False)\n",
    "        end = time.time()\n",
    "        elapsed_time.append(int(end - start))\n",
    "        total_avg_bias.append(evaluate_avg_bias(source_ips, count_min_sketch, true_frequencies))\n",
    "        elephant_flows_avg_bias.append(evaluate_avg_bias(top_elephant_flows, count_min_sketch, true_frequencies))\n",
    "        rare_flows_avg_bias.append(evaluate_avg_bias(top_rare_flows, count_min_sketch, true_frequencies))\n",
    "        empiric_accuracy, calculated_accuracy = calc_count_min_sketch_theoretical_guarantees_accuracy(source_ips, count_min_sketch, true_frequencies)\n",
    "        theoretical_guarantee_empiric_accuracies.append(empiric_accuracy)\n",
    "        theoretical_guarantee_calculated_accuracies.append(calculated_accuracy)\n",
    "        results_dict = {'d_w_pairs': d_w_pairs,\n",
    "                        'elapsed_time': elapsed_time,\n",
    "                        'total_avg_bias': total_avg_bias,\n",
    "                        'elephant_flows_avg_bias': elephant_flows_avg_bias,\n",
    "                        'rare_flows_avg_bias': rare_flows_avg_bias,\n",
    "                        'theoretical_guarantee_empiric_accuracies': theoretical_guarantee_empiric_accuracies\n",
    "                        'theoretical_guarantee_calculated_accuracies': theoretical_guarantee_calculated_accuracies}\n",
    "        with open('count_min_regular_counters_results.pkl', 'wb') as f:\n",
    "            pickle.dump(results_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_dict).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the best results are received when d=2, w=512 and d=3, w=256. So for the next experiment we'll try values that are closer to these in order to save time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_morris_counters(morris_sigma=0.5, morris_delta=0.9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using morris counters, there's a tradeoff between the amount of matrix counters we use and the amount of morris counters we use to maintain every matrix counter. The storage required for a morris counter is roughly log(64) = 6 bits, therefore a regular counter requires rouhgly the same storage size as 10 morris counters. We don't want to use 10 morris counters per matrix counter because we would prefer using a regular counter and get better accuracy. Our product managers told us that they are willing to tolerate morris_sigma = 0.5 and morris_delta = 0.9, and therefore we can use 4 morris counters per matrix counter. 4 morris counters are 3 bytes, therefore we can use 2730 matrix counters:\n",
    " 1. d = 2, w = 1365\n",
    " 2. d = 4, w = 682\n",
    "\n",
    "Note that for d = 2 we can use w = 1365 matrix counters which is more than twice the amount of regular matrix counters we could use with the same storage capacity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile('count_min_morris_counters_results.pkl'):\n",
    "    with open('count_min_morris_counters_results.pkl', 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "else:\n",
    "    results_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not results_dict:\n",
    "    d_w_pairs = [(2, 1365), (4, 682)]\n",
    "    elapsed_time = []\n",
    "    total_avg_bias = []\n",
    "    elephant_flows_avg_bias = []\n",
    "    rare_flows_avg_bias = []\n",
    "    theoretical_guarantee_empiric_accuracies = []\n",
    "    theoretical_guarantee_calculated_accuracies = []\n",
    "    morris_sigma = 0.5\n",
    "    morris_delta = 0.9\n",
    "    start = time.time()\n",
    "    for d, w in d_w_pairs:\n",
    "        print(f'calculating count min sketch with regular counters with d={d} and w={w} and {count_morris_counters(morris_sigma, morris_delta)} morris counters per matrix counter')\n",
    "        start = time.time()\n",
    "        count_min_sketch = CountMinSketch(source_ips, d, w, morris_sigma, morris_delta)\n",
    "        end = time.time()\n",
    "        elapsed_time.append(int(end - start))\n",
    "        total_avg_bias.append(evaluate_avg_bias(source_ips, count_min_sketch, true_frequencies))\n",
    "        elephant_flows_avg_bias.append(evaluate_avg_bias(top_elephant_flows, count_min_sketch, true_frequencies))\n",
    "        rare_flows_avg_bias.append(evaluate_avg_bias(top_rare_flows, count_min_sketch, true_frequencies))\n",
    "        empiric_accuracy, calculated_accuracy = calc_count_min_sketch_theoretical_guarantees_accuracy(source_ips, count_min_sketch, true_frequencies)\n",
    "        theoretical_guarantee_empiric_accuracies.append(empiric_accuracy)\n",
    "        theoretical_guarantee_calculated_accuracies.append(calculated_accuracy)\n",
    "    results_dict = {'d_w_pairs': d_w_pairs,\n",
    "                    'elapsed_time': elapsed_time,\n",
    "                    'total_avg_bias': total_avg_bias,\n",
    "                    'elephant_flows_avg_bias': elephant_flows_avg_bias,\n",
    "                    'rare_flows_avg_bias': rare_flows_avg_bias,\n",
    "                    'theoretical_guarantee_empiric_accuracies': theoretical_guarantee_empiric_accuracies\n",
    "                    'theoretical_guarantee_calculated_accuracies': theoretical_guarantee_calculated_accuracies}\n",
    "    with open('count_min_morris_counters_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_dict).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the best results are received when d=2, w=1365.\n",
    "\n",
    "The result is 4 times more accurate than the best result we got using regular matrix counters with the same storage capacity, however it also ran 1.5 times slower than with regular matrix counters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_sign_hash_functions(stream, d):\n",
    "    # Generates perfect sign hash functions from source ips to values in {1, -1}.\n",
    "    sign_hash_functions = []\n",
    "    for _ in range(d):\n",
    "        sign_hash_functions.append(dict(map(lambda key, value: (key, value), stream.unique(), np.random.choice([1, -1], len(stream.unique())))))\n",
    "    return sign_hash_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count Sketch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CountSketch:\n",
    "    def __init__(self, stream, d, w, morris_sigma=0.5, morris_delta=0.9, use_morris_counters=True):\n",
    "        self.d = d\n",
    "        self.w = w\n",
    "        self.morris_estimators_num = count_morris_counters(morris_sigma, morris_delta)\n",
    "        self.use_morris_counters = use_morris_counters\n",
    "        self.hash_functions = init_hash_functions(stream, d, w)\n",
    "        self.sign_hash_functions = init_sign_hash_functions(stream, d)\n",
    "        self.sketch = self.create_sketch(stream)\n",
    "\n",
    "    def create_sketch(self, stream):\n",
    "        sketch = {}\n",
    "        for i, item in enumerate(stream):\n",
    "            for j, hash_function in enumerate(self.hash_functions):\n",
    "                sign_hash_function = self.sign_hash_functions[j]\n",
    "                if self.use_morris_counters:\n",
    "                    morris_estimators = sketch.get((j, hash_function[item]), np.zeros(self.morris_estimators_num))\n",
    "                    p = 1 / (2 ** morris_estimators)\n",
    "                    update_mask = np.random.uniform(0, 1, size=morris_estimators.shape) <= p\n",
    "                    morris_estimators[update_mask] += sign_hash_function[item]\n",
    "                    sketch[(j, hash_function[item])] = morris_estimators\n",
    "                else:\n",
    "                    sketch[(j, hash_function[item])] = sketch.get((j, hash_function[item]), 0) + sign_hash_function[item]\n",
    "            sys.stdout.write(f'\\rprocessing element {i} of {len(stream)}: {round(float(i / len(stream)) * 100, 2)}% completed.')\n",
    "        print()\n",
    "        return sketch\n",
    "\n",
    "    def query(self, item):\n",
    "        if self.use_morris_counters:\n",
    "            hash_values = []\n",
    "            for i, hash_function in enumerate(self.hash_functions):\n",
    "                morris_estimators = self.sketch[(i, hash_function[item])]\n",
    "                processed_morris_estimators = (2 ** morris_estimators) - 1\n",
    "                hash_values.append(np.mean(processed_morris_estimators) * self.sign_hash_functions[i])\n",
    "            return np.median(hash_values)\n",
    "        else:\n",
    "            hash_values = [self.sketch[(i, hash_function[item])] * self.sign_hash_functions[i][item] for i, hash_function in enumerate(self.hash_functions)]\n",
    "            return np.median(hash_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function calculates whether the theoretical guarantee for count sketch holds by counting the number of elements that are in the range and comparing to the total number of elements:\n",
    "\n",
    "If $w = \\dfrac{3}{\\epsilon^2}$ and $d = log\\dfrac{1}{\\delta}$ then $Pr[|\\tilde{f}_x - f_x| \\ge \\epsilon ||f||_2] \\ge 1 - \\delta$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_count_sketch_theoretical_guarantees_accuracy(items, sketch, true_frequencies):\n",
    "    unique_items = items.unique()\n",
    "    items_that_hold_range_guarantee = 0\n",
    "    epsilon = math.sqrt(float(3 / sketch.w))\n",
    "    delta = float(1 / (2 ** sketch.d))\n",
    "    frequencies_norm = np.linalg.norm(true_frequencies.values())\n",
    "    for item in unique_items:\n",
    "        true_frequency = true_frequencies[item]\n",
    "        estimated_frequency = sketch.query(item)\n",
    "        if abs(estimated_frequency - true_frequency) >= epsilon * frequencies_norm:\n",
    "            items_that_hold_range_guarantee += 1\n",
    "    return round(float(items_that_hold_range_guarantee / len(unique_items)), 2), 1 - delta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The analysis we will perform on the count sketch is similar to the analysis we did for count min sketch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile('count_regular_counters_results.pkl'):\n",
    "    with open('count_regular_counters_results.pkl', 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "else:\n",
    "    results_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not results_dict:\n",
    "    d_w_pairs = [(1, 1024), (2, 512), (4, 256), (8, 128)]\n",
    "    elapsed_time = []\n",
    "    total_avg_bias = []\n",
    "    elephant_flows_avg_bias = []\n",
    "    rare_flows_avg_bias = []\n",
    "    theoretical_guarantee_empiric_accuracies = []\n",
    "    theoretical_guarantee_calculated_accuracies = []\n",
    "    for d, w in d_w_pairs:\n",
    "        print(f'calculating count sketch with regular counters with d={d} and w={w}')\n",
    "        start = time.time()\n",
    "        count_sketch = CountSketch(source_ips, d, w, use_morris_counters=False)\n",
    "        end = time.time()\n",
    "        elapsed_time.append(int(end - start))\n",
    "        total_avg_bias.append(evaluate_avg_bias(source_ips, count_sketch, true_frequencies))\n",
    "        elephant_flows_avg_bias.append(evaluate_avg_bias(top_elephant_flows, count_sketch, true_frequencies))\n",
    "        rare_flows_avg_bias.append(evaluate_avg_bias(top_rare_flows, count_sketch, true_frequencies))\n",
    "        empiric_accuracy, calculated_accuracy = calc_count_sketch_theoretical_guarantees_accuracy(source_ips, count_sketch, true_frequencies)\n",
    "        theoretical_guarantee_empiric_accuracies.append(empiric_accuracy)\n",
    "        theoretical_guarantee_calculated_accuracies.append(calculated_accuracy)\n",
    "    results_dict = {'d_w_pairs': d_w_pairs,\n",
    "                    'elapsed_time': elapsed_time,\n",
    "                    'total_avg_bias': total_avg_bias,\n",
    "                    'elephant_flows_avg_bias': elephant_flows_avg_bias,\n",
    "                    'rare_flows_avg_bias': rare_flows_avg_bias,\n",
    "                    'theoretical_guarantee_empiric_accuracies': theoretical_guarantee_empiric_accuracies\n",
    "                    'theoretical_guarantee_calculated_accuracies': theoretical_guarantee_calculated_accuracies}\n",
    "    with open('count_regular_counters_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_dict).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.isfile('count_morris_counters_results.pkl'):\n",
    "    with open('count_morris_counters_results.pkl', 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "else:\n",
    "    results_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not results_dict:\n",
    "    d_w_pairs = [(2, 1365), (4, 682)]\n",
    "    elapsed_time = []\n",
    "    total_avg_bias = []\n",
    "    elephant_flows_avg_bias = []\n",
    "    rare_flows_avg_bias = []\n",
    "    theoretical_guarantee_empiric_accuracies = []\n",
    "    theoretical_guarantee_calculated_accuracies = []\n",
    "    morris_sigma = 0.5\n",
    "    morris_delta = 0.9\n",
    "    start = time.time()\n",
    "    for d, w in d_w_pairs:\n",
    "        print(f'calculating count sketch with regular counters with d={d} and w={w} and {count_morris_counters(morris_sigma, morris_delta)} morris counters per matrix counter')\n",
    "        start = time.time()\n",
    "        count_sketch = CountSketch(source_ips, d, w, morris_sigma, morris_delta)\n",
    "        end = time.time()\n",
    "        elapsed_time.append(int(end - start))\n",
    "        total_avg_bias.append(evaluate_avg_bias(source_ips, count_sketch, true_frequencies))\n",
    "        elephant_flows_avg_bias.append(evaluate_avg_bias(top_elephant_flows, count_sketch, true_frequencies))\n",
    "        rare_flows_avg_bias.append(evaluate_avg_bias(top_rare_flows, count_sketch, true_frequencies))\n",
    "        empiric_accuracy, calculated_accuracy = calc_count_sketch_theoretical_guarantees_accuracy(source_ips, count_sketch, true_frequencies)\n",
    "        theoretical_guarantee_empiric_accuracies.append(empiric_accuracy)\n",
    "        theoretical_guarantee_calculated_accuracies.append(calculated_accuracy)\n",
    "    results_dict = {'d_w_pairs': d_w_pairs,\n",
    "                    'elapsed_time': elapsed_time,\n",
    "                    'total_avg_bias': total_avg_bias,\n",
    "                    'elephant_flows_avg_bias': elephant_flows_avg_bias,\n",
    "                    'rare_flows_avg_bias': rare_flows_avg_bias,\n",
    "                    'theoretical_guarantee_empiric_accuracies': theoretical_guarantee_empiric_accuracies\n",
    "                    'theoretical_guarantee_calculated_accuracies': theoretical_guarantee_calculated_accuracies}\n",
    "    with open('count_morris_counters_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results_dict).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}