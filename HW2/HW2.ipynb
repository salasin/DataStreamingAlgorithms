{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Streaming Algorithms\n",
    "\n",
    "## HW2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See preprocessed notebook at https://github.com/salasin/DataStreamingAlgorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dry Part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$M$ is a data stream and $|M| = n$.\n",
    "\n",
    "$A$ is $M$'s heavy hitters sketch and $|A| = k$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) For every $j ∈ A, \\tilde{f_j} \\ge f_j$\n",
    "\n",
    "The proof is by induction on the number of appearances of an element in the stream.\n",
    "\n",
    "**Base case:**\n",
    "\n",
    "After the 1st appearance of $j$ we have $f_j = 1$.\n",
    "\n",
    "If $|A| < k$ before $j$'s appearance then it's added to the sketch and we get $\\tilde{f_j} = 1 \\ge f_j$.\n",
    "\n",
    "Otherwise, if $|A| = k$ then we get $\\tilde{f_j} = 1 + \\tilde{f_{}}_{min} \\ge 1 = f_j$ because $\\tilde{f_{}}_{min} \\ge 0$ because the counters are non-negative.\n",
    "\n",
    "**Induction step:**\n",
    "\n",
    "We denote by $\\tilde{f_j}^{(i)}$ the value of $j$'s counter after $i$ appearances of $j$ and by $f_j^{(i)}$ the true count of $j$'s appearances (which is $i$).\n",
    "\n",
    "We assume that $\\tilde{f_j}^{(i)} \\ge f_j^{(i)}$ holds for a given $i \\ge 1$.\n",
    "\n",
    "Then for $i + 1$ we get:\n",
    "\n",
    "If $j \\in A$ then we incremenet $\\tilde{f_j}^{(i)}$ and we get $\\tilde{f_j}^{(i + 1)} = \\tilde{f_j}^{(i)} + 1 \\ge f_j^{(i)} + 1 = f_j^{(i + 1)}$.\n",
    "\n",
    "If $j \\notin A$ and since $i \\ge 1$ (not $j$'s first appearance) we can infer that $j$ was in $A$ and was removed. Therefore it must hold that $|A| = k$. At the time of $j$'s removal from $A$ we had $\\tilde{f_j}^{(i)} = \\tilde{f_{}}_{min}$. Now we want to re-insert $j$ to $A$ and we have a new $\\tilde{f_{}}_{min}^{(new)}$ that is bigger than $\\tilde{f_{}}_{min}$ because all counters are monotonically increasing. Then we get $\\tilde{f_j}^{(i + 1)} = \\tilde{f_{}}_{min}^{(new)} + 1 > \\tilde{f_{}}_{min} + 1 = \\tilde{f_j}^{(i)} + 1 \\ge f_j^{(i)} + 1 = f_j^{(i + 1)}$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) For every $j \\in [M], \\tilde{f_j} ≤ f_j + \\tilde{f_{}}_{min}$\n",
    "\n",
    "If $j \\notin A$ we have $\\tilde{f_j} = 0$ and the inequality holds since the right side is non-negative.\n",
    "\n",
    "If $j \\in A$, then $j$ was inserted to $A$ at some point and additional instances of $j$ may have been observed since. We denote with $\\tilde{f_j}^{(curr)}$ the current value of the counter corresponding to $j$. We denote with $\\tilde{f_{}}_{min}^{(curr)}$ the value of the counter that currently has the minimal value and with $\\tilde{f_{}}_{min}^{(old)}$ the value of the counter that had the minimal value at the time $j$ was inserted. We denote with $f_{j}^{(before)}$ the count of $j$ instances before $j$'s insertion and with $f_{j}^{(after)}$ the similar count after the insertion.\n",
    "\n",
    "We observe that $f_j = f_{j}^{(before)} + f_{j}^{(after)}$ and therefore $f_j \\ge f_{j}^{(after)}$. We also observe that $\\tilde{f_{}}_{min}^{(old)} \\le \\tilde{f_{}}_{min}^{(curr)}$ because all counters are monotonically increasing.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\tilde{f_j} = \\tilde{f_j}^{(curr)} = f_{j}^{(after)} + \\tilde{f_{}}_{min}^{(old)} \\le f_j + \\tilde{f_{}}_{min}^{(curr)} = f_j + \\tilde{f_{}}_{min}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(c) $ֿ\\sum_{j \\in A} \\tilde{f_j} = n$\n",
    "\n",
    "Proof by induction on the size of the stream.\n",
    "\n",
    "**Base case:**\n",
    "\n",
    "When n = 1, i.e. when we process the first element $i$ in the stream, we add a new counter $\\tilde{f_i} = 1$ to the sketch. Therefore:\n",
    "\n",
    "$ֿ\\sum_{j \\in A} \\tilde{f_j} = \\tilde{f_i} = 1 = n$\n",
    "\n",
    "**Induction step:**\n",
    "\n",
    "We denote by $\\sum_{j \\in A}^{(n)} \\tilde{f_j}$ the sum of all counters and by $\\tilde{f_{j}}^{(n)}$ the value of $j$'s counter after $n$ elements from the stream were processed.\n",
    "\n",
    "We assume that $ֿ\\sum_{j \\in A}^{(n)} \\tilde{f_j} = n$ holds for a given $n$.\n",
    "\n",
    "Then for $n + 1$ we get:\n",
    "\n",
    "We start processing the $n + 1$ element $i$ so we incremenet $\\tilde{f_i}$ (Note that it doesn't matter in this context whether $i$ was in $A$. If it wasn't we simply assume that before the incrementation $\\tilde{f_i} = \\tilde{f}_{min}$) and we get:\n",
    "\n",
    " $\\sum_{j \\in A}^{(n + 1)} \\tilde{f_j} = ֿ\\sum_{j \\in A, j \\neq i}^{(n + 1)} \\tilde{f_j} + \\tilde{f_i}^{(n + 1)} = \\sum_{j \\in A, j \\neq i}^{(n + 1)} \\tilde{f_j} + \\tilde{f_i}^{(n)} + 1 = \\sum_{j \\in A}^{(n)} \\tilde{f_j} + 1 = n + 1$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(d) $\\tilde{f}_{min} \\le \\lfloor n/k \\rfloor$ and hence $f_j \\le \\tilde{f_j} \\le f_j + \\lfloor n/k \\rfloor$ for every $j \\in A$.\n",
    "\n",
    "From (c) we know that $ֿ\\sum_{j \\in A} \\tilde{f_j} = n$. Also, the number of counters is $k$. Therefore, the average value of a counter is $\\lfloor n / k \\rfloor$. Since $\\tilde{f}_{min}$ is the smallest of all counters then it must be below the average and therefore $\\tilde{f}_{min} \\le \\lfloor n/k \\rfloor$.\n",
    "\n",
    "Combining this conclusion with the conclusions from (a) and (b) we can deduce that $f_j \\le \\tilde{f_j} \\le f_j + \\lfloor n/k \\rfloor$ for every $j \\in A$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wet Part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numer of unique source ips: 9640\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('2017-07-03.csv')\n",
    "source_ips = data.loc[:, 'ipv4Src']\n",
    "unique_source_ips = len(source_ips.unique())\n",
    "print(f'numer of unique source ips: {unique_source_ips}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true frequency of 35.160.100.86 is: 753\n"
     ]
    }
   ],
   "source": [
    "true_frequencies = {}\n",
    "for source_ip in source_ips:\n",
    "    true_frequencies[source_ip] = true_frequencies.get(source_ip, 0) + 1\n",
    "print(f'true frequency of 35.160.100.86 is: {true_frequencies[\"35.160.100.86\"]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def init_hash_functions(d, w):\n",
    "    # Generates perfect hash functions from source ips to values between 0 to w - 1.\n",
    "    hash_functions = []\n",
    "    for _ in range(d):\n",
    "        hash_functions.append(dict(map(lambda key, value: (key, value), source_ips.unique(), np.random.randint(0, w, len(source_ips.unique())))))\n",
    "    return hash_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def count_morris_counters(morris_sigma, morris_delta):\n",
    "    return int(1 / (morris_delta * (morris_sigma ** 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count Min Sketch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class CountMinSketch:\n",
    "    def __init__(self, stream, d, w, morris_sigma=0.2, morris_delta=0.9, use_morris_counters=True):\n",
    "        self.d = d\n",
    "        self.w = w\n",
    "        self.morris_estimators_num = count_morris_counters(morris_sigma, morris_delta)\n",
    "        self.use_morris_counters = use_morris_counters\n",
    "        self.hash_functions = init_hash_functions(d, w)\n",
    "        self.sketch = self.create_sketch(stream)\n",
    "\n",
    "    def create_sketch(self, stream):\n",
    "        sketch = {}\n",
    "        for i, item in enumerate(stream):\n",
    "            for j, hash_function in enumerate(self.hash_functions):\n",
    "                if self.use_morris_counters:\n",
    "                    morris_estimators = sketch.get((j, hash_function[item]), np.zeros(self.morris_estimators_num))\n",
    "                    p = 1 / (2 ** morris_estimators)\n",
    "                    update_mask = np.random.uniform(0, 1, size=morris_estimators.shape) <= p\n",
    "                    morris_estimators[update_mask] += 1\n",
    "                    sketch[(j, hash_function[item])] = morris_estimators\n",
    "                else:\n",
    "                    sketch[(j, hash_function[item])] = sketch.get((j, hash_function[item]), 0) + 1\n",
    "            sys.stdout.write(f'\\rprocessing element {i} of {len(stream)}: {round(float(i / len(stream)) * 100, 2)}% completed.')\n",
    "        print()\n",
    "        return sketch\n",
    "\n",
    "    def query(self, item):\n",
    "        if self.use_morris_counters:\n",
    "            hash_values = []\n",
    "            for i, hash_function in enumerate(self.hash_functions):\n",
    "                morris_estimators = self.sketch[(i, hash_function[item])]\n",
    "                processed_morris_estimators = (2 ** morris_estimators) - 1\n",
    "                hash_values.append(np.mean(processed_morris_estimators))\n",
    "            return min(hash_values)\n",
    "        else:\n",
    "            hash_values = [self.sketch[(i, hash_function[item])] for i, hash_function in enumerate(self.hash_functions)]\n",
    "            return min(hash_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def evaluate_avg_bias(items, sketch, true_frequencies):\n",
    "    unique_items = items.unique()\n",
    "    bias_per_item = []\n",
    "    for item in unique_items:\n",
    "        true_frequency = true_frequencies[item]\n",
    "        estimated_frequency = sketch.query(item)\n",
    "        bias = float(estimated_frequency / true_frequency)\n",
    "        normalized_bias = 1 - bias if bias <= 1 else bias - 1\n",
    "        bias_per_item.append(normalized_bias)\n",
    "    return round(sum(bias_per_item) / len(bias_per_item), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to focus the problem we have, we assume that we're given a business requirement to reduce the memory consumption to 10% of the solution that uses true counters.\n",
    "\n",
    "Since we have 9640 unique elements in the stream and assuming a single counter size is 8 bytes, we need 77,120 bytes to store all the counters. We will try to reduce it to 8KB = 8192 bytes which approximately meets the business requirements.\n",
    "\n",
    "We will try to optimize the sketch accuracy and overall runtime using only 8 KB."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using regular counters, we can use up to 2**10 counters:\n",
    "1. d = 1, w = 1024\n",
    "2. d = 2, w = 512\n",
    "3. d = 4, w = 256\n",
    "4. d = 8, w = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "if os.path.isfile('count_min_regular_counters_results.pkl'):\n",
    "    with open('count_min_regular_counters_results.pkl', 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "else:\n",
    "    results_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "if not results_dict:\n",
    "    d_w_pairs = [(1, 1024), (2, 512), (4, 256), (8, 128)]\n",
    "    elapsed_time = []\n",
    "    avg_bias = []\n",
    "    for d, w in d_w_pairs:\n",
    "        print(f'calculating count min sketch with regular counters with d={d} and w={w}')\n",
    "        start = time.time()\n",
    "        count_min_sketch = CountMinSketch(source_ips, d, w, use_morris_counters=False)\n",
    "        end = time.time()\n",
    "        elapsed_time.append(int(end - start))\n",
    "        avg_bias.append(evaluate_avg_bias(source_ips, count_min_sketch, true_frequencies))\n",
    "    results_dict = {'d_w_pairs': d_w_pairs, 'elapsed_time': elapsed_time, 'avg_bias': avg_bias}\n",
    "    with open('count_min_regular_counters_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "   elapsed_time  avg_bias  d_w_pairs\n0           837    392.04  (1, 1024)\n1           874    119.05   (2, 512)\n2           914    194.05   (4, 256)\n3           921    404.78   (8, 128)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>elapsed_time</th>\n      <th>avg_bias</th>\n      <th>d_w_pairs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>837</td>\n      <td>392.04</td>\n      <td>(1, 1024)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>874</td>\n      <td>119.05</td>\n      <td>(2, 512)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>914</td>\n      <td>194.05</td>\n      <td>(4, 256)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>921</td>\n      <td>404.78</td>\n      <td>(8, 128)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(results_dict).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the best results are received when d=2, w=512 and d=3, w=256. So for the next experiment we'll try values that are closer to these in order to save time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(count_morris_counters(morris_sigma=0.5, morris_delta=0.9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using morris counters, there's a tradeoff between the amount of matrix counters we use and the amount of morris counters we use to maintain every matrix counter. The storage required for a morris counter is roughly log(64) = 6 bits, therefore a regular counter requires rouhgly the same storage size as 10 morris counters. We don't want to use 10 morris counters per matrix counter because we would prefer using a regular counter and get better accuracy. Our product managers told us that they are willing to tolerate morris_sigma = 0.5 and morris_delta = 0.9, and therefore we can use 4 morris counters per matrix counter. 4 morris counters are 3 bytes, therefore we can use 2730 matrix counters:\n",
    " 1. d = 2, w = 1365\n",
    " 2. d = 4, w = 682\n",
    "\n",
    "Note that for d = 2 we can use w = 1365 matrix counters which is more than twice the amount of regular matrix counters we could use with the same storage capacity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "if os.path.isfile('count_min_morris_counters_results.pkl'):\n",
    "    with open('count_min_morris_counters_results.pkl', 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "else:\n",
    "    results_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "if not results_dict:\n",
    "    d_w_pairs = [(2, 1365), (4, 682)]\n",
    "    elapsed_time = []\n",
    "    avg_bias = []\n",
    "    morris_sigma = 0.5\n",
    "    morris_delta = 0.9\n",
    "    start = time.time()\n",
    "    for d, w in d_w_pairs:\n",
    "        print(f'calculating count min sketch with regular counters with d={d} and w={w} and {count_morris_counters(morris_sigma, morris_delta)} morris counters per matrix counter')\n",
    "        start = time.time()\n",
    "        count_min_sketch = CountMinSketch(source_ips, d, w, morris_sigma, morris_delta)\n",
    "        end = time.time()\n",
    "        elapsed_time.append(int(end - start))\n",
    "        avg_bias.append(evaluate_avg_bias(source_ips, count_min_sketch, true_frequencies))\n",
    "    results_dict = {'d_w_pairs': d_w_pairs, 'elapsed_time': elapsed_time, 'avg_bias': avg_bias}\n",
    "    with open('count_min_morris_counters_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "   d_w_pairs  elapsed_time  avg_bias\n0  (2, 1365)          1269     30.13\n1   (4, 682)          1615     43.45",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>d_w_pairs</th>\n      <th>elapsed_time</th>\n      <th>avg_bias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(2, 1365)</td>\n      <td>1269</td>\n      <td>30.13</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(4, 682)</td>\n      <td>1615</td>\n      <td>43.45</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(results_dict).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the best results are received when d=2, w=1365.\n",
    "\n",
    "The result is 4 times more accurate than the best result we got using regular matrix counters with the same storage capacity, however it also ran 1.5 times slower than with regular matrix counters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Count Sketch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}